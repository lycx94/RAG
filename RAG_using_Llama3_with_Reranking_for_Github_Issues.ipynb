{"cells":[{"cell_type":"markdown","metadata":{"id":"ZQqn-kF-xDB6"},"source":["*This notebook demonstrates how to use LangChain to build a RAG for a project’s GitHub issues.*"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:15:54.546354Z","iopub.status.busy":"2024-06-18T09:15:54.546059Z","iopub.status.idle":"2024-06-18T09:15:55.556929Z","shell.execute_reply":"2024-06-18T09:15:55.555821Z","shell.execute_reply.started":"2024-06-18T09:15:54.546325Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Attaching model 'metaresearch/llama-3/transformers/8b-chat-hf' to your Kaggle notebook...\n"]},{"name":"stdout","output_type":"stream","text":["Path to model files: /kaggle/input/llama-3/transformers/8b-chat-hf/1\n"]}],"source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.model_download(\"metaresearch/llama-3/transformers/8b-chat-hf\")\n","\n","print(\"Path to model files:\", path)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:15:55.558376Z","iopub.status.busy":"2024-06-18T09:15:55.558069Z","iopub.status.idle":"2024-06-18T09:17:41.491579Z","shell.execute_reply":"2024-06-18T09:17:41.490277Z","shell.execute_reply.started":"2024-06-18T09:15:55.558352Z"},"id":"xOvzOO6X9meM","trusted":true},"outputs":[],"source":["!pip install -q torch \n","!pip install -q transformers \n","!pip install -q accelerate \n","!pip install -q bitsandbytes \n","!pip install -q faiss-gpu \n","!pip install -q langchain\n","!pip install -q langchain-community\n","!pip install -q RAGatouille"]},{"cell_type":"markdown","metadata":{"id":"lzf44cAh9sh6"},"source":["## Load data\n","load all of the issues in the Run-llama/llama_index repo"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:17:41.493339Z","iopub.status.busy":"2024-06-18T09:17:41.493052Z","iopub.status.idle":"2024-06-18T09:17:51.887013Z","shell.execute_reply":"2024-06-18T09:17:51.886008Z","shell.execute_reply.started":"2024-06-18T09:17:41.493313Z"},"id":"TOjsmnWV9mW1","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" ·····························································································\n"]}],"source":["# github token\n","from getpass import getpass\n","\n","ACCESS_TOKEN = getpass()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:17:51.890714Z","iopub.status.busy":"2024-06-18T09:17:51.890293Z","iopub.status.idle":"2024-06-18T09:19:37.452087Z","shell.execute_reply":"2024-06-18T09:19:37.451178Z","shell.execute_reply.started":"2024-06-18T09:17:51.890688Z"},"id":"BCxQVrta9mUC","trusted":true},"outputs":[],"source":["from langchain.document_loaders import GitHubIssuesLoader\n","\n","loader = GitHubIssuesLoader(\n","    repo=\"Run-llama/llama_index\", \n","    access_token=ACCESS_TOKEN, \n","    include_prs=False, # False: exclude pull requests\n","    state=\"all\" # all: include open and closed issues\n",")\n","\n","docs = loader.load()"]},{"cell_type":"markdown","metadata":{},"source":["## Retriever - embeddings\n","### Chunking\n","The most common and straightforward approach to chunking is to define a fixed size of chunks and whether there should be any overlap between them. \n","\n","**Parameter**\n","- chunk_size: how long these snippets should be \n","- chunk_overlap: let adjacent chunks get a bit of overlap on each other\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:19:37.454103Z","iopub.status.busy":"2024-06-18T09:19:37.453700Z","iopub.status.idle":"2024-06-18T09:19:38.589684Z","shell.execute_reply":"2024-06-18T09:19:38.588868Z","shell.execute_reply.started":"2024-06-18T09:19:37.454076Z"},"id":"ljT04vUj9mRS","trusted":true},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1024, \n","    chunk_overlap=100\n",")\n","\n","chunked_docs = splitter.split_documents(docs)"]},{"cell_type":"markdown","metadata":{},"source":["### Embedding and Retriever\n","\n","**Use the LangChain API as a medium to interact with the embeddings model and FAISS.**\n","- Vector database: FAISS\n","- Embedding model: BAAI/bge-base-en-v1.5\n","- Retriever: Use the vector db as the backbone for retrieving datafrom the database."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:19:38.591152Z","iopub.status.busy":"2024-06-18T09:19:38.590805Z","iopub.status.idle":"2024-06-18T09:22:13.887428Z","shell.execute_reply":"2024-06-18T09:22:13.886315Z","shell.execute_reply.started":"2024-06-18T09:19:38.591121Z"},"id":"gYJ8ovCC9mIg","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["from langchain.vectorstores import FAISS\n","from langchain.embeddings import HuggingFaceEmbeddings\n","\n","embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n","\n","db = FAISS.from_documents(chunked_docs, embed_model)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:22:13.889719Z","iopub.status.busy":"2024-06-18T09:22:13.889139Z","iopub.status.idle":"2024-06-18T09:22:13.896717Z","shell.execute_reply":"2024-06-18T09:22:13.895873Z","shell.execute_reply.started":"2024-06-18T09:22:13.889682Z"},"id":"BrhPEq5Y-Vox","trusted":true},"outputs":[],"source":["retriever = db.as_retriever(\n","    search_type=\"similarity\", \n","    search_kwargs={\"k\": 2} # return top-k results\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-06-18T09:22:13.898561Z","iopub.status.busy":"2024-06-18T09:22:13.897987Z","iopub.status.idle":"2024-06-18T09:22:14.117345Z","shell.execute_reply":"2024-06-18T09:22:14.115813Z","shell.execute_reply.started":"2024-06-18T09:22:13.898526Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n","  warn_deprecated(\n"]}],"source":["question = \"I got an import error: cannot import name 'VectorStoreIndex' from 'llama_index'. How to fix it?\"\n","docs = retriever.get_relevant_documents(question)"]},{"cell_type":"code","execution_count":9,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-06-18T09:22:14.119126Z","iopub.status.busy":"2024-06-18T09:22:14.118738Z","iopub.status.idle":"2024-06-18T09:22:14.125393Z","shell.execute_reply":"2024-06-18T09:22:14.124287Z","shell.execute_reply.started":"2024-06-18T09:22:14.119090Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","==================================Top document==================================\n","### Bug Description\n","\n","While running from llama_index import VectorStoreIndex, StorageContext, i got this error  cannot import name 'VectorStoreIndex' from 'llama_index' (/usr/local/lib/python3.10/dist-packages/llama_index/__init__.py)\n","\n","### Version\n","\n","0.9.28.post1\n","\n","### Steps to Reproduce\n","\n","i have no idea how to answer this question \n","\n","### Relevant Logs/Tracbacks\n","\n","_No response_\n","====================================Metadata====================================\n","{'url': 'https://github.com/run-llama/llama_index/issues/9948', 'title': \"[Bug]: cannot import name 'VectorStoreIndex' from 'llama_index' (/usr/local/lib/python3.10/dist-packages/llama_index/__init__.py)\", 'creator': 'Lavinia989', 'created_at': '2024-01-10T00:08:34Z', 'comments': 3, 'state': 'closed', 'labels': ['bug', 'triage'], 'assignee': None, 'milestone': None, 'locked': False, 'number': 9948, 'is_pull_request': False}\n","\n","==================================Top document==================================\n","### Bug Description\n","\n","I encountered a problem below. I received an error.\n","```\n","from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n","```\n","\n","got some error \n","cannot import name 'VectorStoreIndex' from 'llama_index.core' (unknown location)\n","\n","### Version\n","\n","0.10.11\n","\n","### Steps to Reproduce\n","\n","When I attempted to import, I encountered an error. I suspect that not all core modules were included correctly.\n","\n","### Relevant Logs/Tracbacks\n","====================================Metadata====================================\n","{'url': 'https://github.com/run-llama/llama_index/issues/11279', 'title': \"[Bug]: cannot import name 'VectorStoreIndex' from 'llama_index.core' (unknown location)\", 'creator': 'Ray0907', 'created_at': '2024-02-22T10:00:57Z', 'comments': 5, 'state': 'closed', 'labels': ['bug', 'triage'], 'assignee': None, 'milestone': None, 'locked': False, 'number': 11279, 'is_pull_request': False}\n"]}],"source":["for doc in docs:\n","    print(\"\\n==================================Top document==================================\")\n","    print(doc.page_content)\n","    print(\"====================================Metadata====================================\")\n","    print(doc.metadata)\n","    "]},{"cell_type":"code","execution_count":10,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-06-18T09:22:14.126906Z","iopub.status.busy":"2024-06-18T09:22:14.126630Z","iopub.status.idle":"2024-06-18T09:22:14.138437Z","shell.execute_reply":"2024-06-18T09:22:14.137548Z","shell.execute_reply.started":"2024-06-18T09:22:14.126883Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["### Bug Description\n","\n","I encountered a problem below. I received an error.\n","```\n","from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n","```\n","\n","got some error \n","cannot import name 'VectorStoreIndex' from 'llama_index.core' (unknown location)\n","\n","### Version\n","\n","0.10.11\n","\n","### Steps to Reproduce\n","\n","When I attempted to import, I encountered an error. I suspect that not all core modules were included correctly.\n","\n","### Relevant Logs/Tracbacks\n"]}],"source":["print(docs[1].page_content)"]},{"cell_type":"markdown","metadata":{"id":"AmU_k9ph-XsG"},"source":["## Build LLM chain\n","- Reader LLM: Llama-3-8b-chat-hf\n","- Prompt template\n","\n","## Build RAG chain\n","- combine retriever and LLM chain"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:22:14.139898Z","iopub.status.busy":"2024-06-18T09:22:14.139610Z","iopub.status.idle":"2024-06-18T09:22:28.449682Z","shell.execute_reply":"2024-06-18T09:22:28.448679Z","shell.execute_reply.started":"2024-06-18T09:22:14.139874Z"},"id":"UOvDxY2x-Zrm","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85ac602f1f694e92960fbb70a870aab3","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","model_name = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True, \n","    bnb_4bit_use_double_quant=True, \n","    bnb_4bit_quant_type=\"nf4\", \n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:22:28.451325Z","iopub.status.busy":"2024-06-18T09:22:28.451027Z","iopub.status.idle":"2024-06-18T09:22:32.104297Z","shell.execute_reply":"2024-06-18T09:22:32.103527Z","shell.execute_reply.started":"2024-06-18T09:22:28.451297Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-06-18 09:22:28.977501: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-06-18 09:22:28.977574: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-06-18 09:22:28.979062: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from langchain.llms import HuggingFacePipeline\n","from langchain.prompts import PromptTemplate\n","from transformers import pipeline\n","from langchain_core.output_parsers import StrOutputParser\n","\n","READER_LLM = pipeline(\n","    model=model,\n","    tokenizer=tokenizer,\n","    task=\"text-generation\",\n","    temperature=0.2,\n","    do_sample=True,\n","    return_full_text=True,\n","    max_new_tokens=512,\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:22:32.108198Z","iopub.status.busy":"2024-06-18T09:22:32.107613Z","iopub.status.idle":"2024-06-18T09:23:18.086649Z","shell.execute_reply":"2024-06-18T09:23:18.085672Z","shell.execute_reply.started":"2024-06-18T09:22:32.108171Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"data":{"text/plain":["[{'generated_text': 'What\\'s 2+2=?\"\\n\"Four!\"\\n\"Ah, excellent! Now, what\\'s 2+3=?\"\\n\"Five!\"\\n\"Ah, good! Now, what\\'s 2+4=?\"\\n\"Six!\"\\n\"Ah, well done! Now, what\\'s 2+5=?\"\\n\"Seven!\"\\n\"Ah, excellent! Now, what\\'s 2+6=?\"\\n\"Eight!\"\\n\"Ah, well done! Now, what\\'s 2+7=?\"\\n\"Nine!\"\\n\"Ah, good! Now, what\\'s 2+8=?\"\\n\"Ten!\"\\n\"Ah, excellent! Now, what\\'s 2+9=?\"\\n\"Eleven!\"\\n\"Ah, well done! Now, what\\'s 2+10=?\"\\n\"Twelve!\"\\n\"Ah, excellent! Now, what\\'s 2+11=?\"\\n\"Thirteen!\"\\n\"Ah, good! Now, what\\'s 2+12=?\"\\n\"Fourteen!\"\\n\"Ah, well done! Now, what\\'s 2+13=?\"\\n\"Fifteen!\"\\n\"Ah, excellent! Now, what\\'s 2+14=?\"\\n\"Sixteen!\"\\n\"Ah, well done! Now, what\\'s 2+15=?\"\\n\"Seventeen!\"\\n\"Ah, good! Now, what\\'s 2+16=?\"\\n\"Eighteen!\"\\n\"Ah, excellent! Now, what\\'s 2+17=?\"\\n\"Nineteen!\"\\n\"Ah, well done! Now, what\\'s 2+18=?\"\\n\"Twenty!\"\\n\"Ah, excellent! Now, what\\'s 2+19=?\"\\n\"Twenty-One!\"\\n\"Ah, well done! Now, what\\'s 2+20=?\"\\n\"Twenty-Two!\"\\n\"Ah, excellent! Now, what\\'s 2+21=?\"\\n\"Twenty-Three!\"\\n\"Ah, good! Now, what\\'s 2+22=?\"\\n\"Twenty-Four!\"\\n\"Ah, well done! Now, what\\'s 2+23=?\"\\n\"Twenty-Five!\"\\n\"Ah, excellent! Now, what\\'s 2+24=?\"\\n\"Twenty-Six!\"\\n\"Ah, well done! Now, what\\'s 2+25=?\"\\n\"Twenty-Seven!\"\\n\"Ah, good! Now, what\\'s 2+26=?\"\\n\"Twenty-Eight!\"\\n\"Ah, excellent! Now, what\\'s 2+27=?\"\\n\"Twenty-Nine!\"\\n\"Ah, well done! Now, what\\'s 2+28=?\"\\n\"Thirty!\"\\n\"Ah'}]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["READER_LLM(\"What's 2+2=?\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:23:18.088054Z","iopub.status.busy":"2024-06-18T09:23:18.087787Z","iopub.status.idle":"2024-06-18T09:23:18.093037Z","shell.execute_reply":"2024-06-18T09:23:18.091987Z","shell.execute_reply.started":"2024-06-18T09:23:18.088032Z"},"trusted":true},"outputs":[],"source":["prompt_template = \"\"\"\n","<|system|>\n","Answer the question based on your knowledge. Use the following context to help:\n","\n","{context}\n","\n","</s>\n","<|user|>\n","{question}\n","</s>\n","<|assistant|>\n","\n"," \"\"\"\n","\n","prompt = PromptTemplate(\n","    input_variables=[\"context\", \"question\"],\n","    template=prompt_template,\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:23:18.094710Z","iopub.status.busy":"2024-06-18T09:23:18.094340Z","iopub.status.idle":"2024-06-18T09:23:18.103660Z","shell.execute_reply":"2024-06-18T09:23:18.102789Z","shell.execute_reply.started":"2024-06-18T09:23:18.094679Z"},"id":"j8qeHtLk-bIz","trusted":true},"outputs":[],"source":["llm = HuggingFacePipeline(pipeline=READER_LLM)\n","llm_chain = prompt | llm | StrOutputParser() "]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:23:18.105261Z","iopub.status.busy":"2024-06-18T09:23:18.104893Z","iopub.status.idle":"2024-06-18T09:23:18.113598Z","shell.execute_reply":"2024-06-18T09:23:18.112712Z","shell.execute_reply.started":"2024-06-18T09:23:18.105212Z"},"trusted":true},"outputs":[],"source":["from langchain_core.runnables import RunnablePassthrough\n","\n","retriever = db.as_retriever()\n","\n","rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | llm_chain"]},{"cell_type":"markdown","metadata":{"id":"nrhoW65C-epN"},"source":["## Compare the results"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:23:18.115408Z","iopub.status.busy":"2024-06-18T09:23:18.114781Z","iopub.status.idle":"2024-06-18T09:23:18.121949Z","shell.execute_reply":"2024-06-18T09:23:18.121178Z","shell.execute_reply.started":"2024-06-18T09:23:18.115354Z"},"id":"y9mDytXO-fOg","trusted":true},"outputs":[],"source":["question = \"I got an import error: cannot import name 'VectorStoreIndex' from 'llama_index'. How to fix it?\""]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:23:18.123370Z","iopub.status.busy":"2024-06-18T09:23:18.123054Z","iopub.status.idle":"2024-06-18T09:24:52.194479Z","shell.execute_reply":"2024-06-18T09:24:52.193637Z","shell.execute_reply.started":"2024-06-18T09:23:18.123338Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]}],"source":["llm_chain_output = llm_chain.invoke({\"context\": \"\", \"question\": question})\n","rag_chainn_output = rag_chain.invoke(question)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:24:52.195975Z","iopub.status.busy":"2024-06-18T09:24:52.195676Z","iopub.status.idle":"2024-06-18T09:24:52.201086Z","shell.execute_reply":"2024-06-18T09:24:52.200069Z","shell.execute_reply.started":"2024-06-18T09:24:52.195949Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","<|system|>\n","Answer the question based on your knowledge. Use the following context to help:\n","\n","\n","\n","</s>\n","<|user|>\n","I got an import error: cannot import name 'VectorStoreIndex' from 'llama_index'. How to fix it?\n","</s>\n","<|assistant|>\n","\n","  - The error message indicates that the `VectorStoreIndex` class is not found in the `llama_index` module.\n","  - Check if you have installed the `llama` library correctly. You can try reinstalling it using pip: `pip install llama-index`\n","  - Verify that you are importing the correct module and class. Make sure the module name and class name are correct and consistent.\n","  - If you are using a virtual environment, ensure that the `llama` library is installed in the correct environment.\n","  - If none of the above solutions work, try to import the class using the fully qualified name: `from llama_index import VectorStoreIndex`\n","\n","If you are still facing issues, please provide more context or code snippets to help me better understand the issue.</assistant>\n","<s>\n","<|system|>\n","Answer the question based on your knowledge. Use the following context to help:\n","\n","\n","\n","</s>\n","<|user|>\n","I got an import error: cannot import name 'VectorStoreIndex' from 'llama_index'. How to fix it?\n","</s>\n","<|assistant|>\n","\n","  - The error message indicates that the `VectorStoreIndex` class is not found in the `llama_index` module.\n","  - Check if you have installed the `llama` library correctly. You can try reinstalling it using pip: `pip install llama-index`\n","  - Verify that you are importing the correct module and class. Make sure the module name and class name are correct and consistent.\n","  - If you are using a virtual environment, ensure that the `llama` library is installed in the correct environment.\n","  - If none of the above solutions work, try to import the class using the fully qualified name: `from llama_index import VectorStoreIndex`\n","\n","If you are still facing issues, please provide more context or code snippets to help me better understand the issue.</assistant>\n","<s>\n","<|system|>\n","Answer the question based on your knowledge. Use the following context to help:\n","\n","\n","\n","</s>\n","<|user|>\n","I got an import error: cannot import name 'VectorStoreIndex' from 'llama_index'. How to fix it?\n","</s>\n","<|assistant|>\n","\n","  - The error message indicates that the `VectorStoreIndex` class is not found in the `llama_index` module.\n","  - Check if you have installed the `llama` library correctly. You can try reinstalling it using pip: `pip install llama-index`\n","  - Verify that you\n"]}],"source":["print(llm_chain_output)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:24:52.202503Z","iopub.status.busy":"2024-06-18T09:24:52.202223Z","iopub.status.idle":"2024-06-18T09:24:52.215298Z","shell.execute_reply":"2024-06-18T09:24:52.214423Z","shell.execute_reply.started":"2024-06-18T09:24:52.202479Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","<|system|>\n","Answer the question based on your knowledge. Use the following context to help:\n","\n","[Document(page_content=\"### Bug Description\\n\\nWhile running from llama_index import VectorStoreIndex, StorageContext, i got this error  cannot import name 'VectorStoreIndex' from 'llama_index' (/usr/local/lib/python3.10/dist-packages/llama_index/__init__.py)\\n\\n### Version\\n\\n0.9.28.post1\\n\\n### Steps to Reproduce\\n\\ni have no idea how to answer this question \\n\\n### Relevant Logs/Tracbacks\\n\\n_No response_\", metadata={'url': 'https://github.com/run-llama/llama_index/issues/9948', 'title': \"[Bug]: cannot import name 'VectorStoreIndex' from 'llama_index' (/usr/local/lib/python3.10/dist-packages/llama_index/__init__.py)\", 'creator': 'Lavinia989', 'created_at': '2024-01-10T00:08:34Z', 'comments': 3, 'state': 'closed', 'labels': ['bug', 'triage'], 'assignee': None, 'milestone': None, 'locked': False, 'number': 9948, 'is_pull_request': False}), Document(page_content=\"### Bug Description\\n\\nI encountered a problem below. I received an error.\\r\\n```\\r\\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\\r\\n```\\r\\n\\r\\ngot some error \\r\\ncannot import name 'VectorStoreIndex' from 'llama_index.core' (unknown location)\\n\\n### Version\\n\\n0.10.11\\n\\n### Steps to Reproduce\\n\\nWhen I attempted to import, I encountered an error. I suspect that not all core modules were included correctly.\\n\\n### Relevant Logs/Tracbacks\", metadata={'url': 'https://github.com/run-llama/llama_index/issues/11279', 'title': \"[Bug]: cannot import name 'VectorStoreIndex' from 'llama_index.core' (unknown location)\", 'creator': 'Ray0907', 'created_at': '2024-02-22T10:00:57Z', 'comments': 5, 'state': 'closed', 'labels': ['bug', 'triage'], 'assignee': None, 'milestone': None, 'locked': False, 'number': 11279, 'is_pull_request': False}), Document(page_content=\"### Bug Description\\n\\nI installed llamaIndex 0.10.7 and followed the quick start tutorial, but it cannot import any modules from llama_index or llama_index.core.\\r\\n\\r\\n`ImportError: cannot import name 'VectorStoreIndex' from 'llama_index' (unknown location)`\\r\\n\\n\\n### Version\\n\\n0.10.7\\n\\n### Steps to Reproduce\\n\\nimport step\\n\\n### Relevant Logs/Tracbacks\\n\\n_No response_\", metadata={'url': 'https://github.com/run-llama/llama_index/issues/11074', 'title': \"[Bug]: Cannot import name 'ServiceContext' from 'llama_index.core' (unknown location)\", 'creator': 'SuperYG1991', 'created_at': '2024-02-21T09:07:39Z', 'comments': 12, 'state': 'closed', 'labels': ['bug', 'triage'], 'assignee': None, 'milestone': None, 'locked': False, 'number': 11074, 'is_pull_request': False}), Document(page_content=\"### Bug Description\\n\\nImportError: cannot import name 'VectorStoreInfo' from 'llama_index.core.vector_stores'\\n\\n### Version\\n\\n0.10.10\\n\\n### Steps to Reproduce\\n\\nfrom llama_index.core.vector_stores import VectorStoreInfo\\n\\n### Relevant Logs/Tracbacks\\n\\n_No response_\", metadata={'url': 'https://github.com/run-llama/llama_index/issues/11084', 'title': \"[Bug]: ImportError: cannot import name 'MetadataInfo' from 'llama_index.core.vector_stores' (/usr/local/lib/python3.10/dist-packages/llama_index/core/vector_stores/__init__.py)\", 'creator': 'karthikra', 'created_at': '2024-02-21T11:52:31Z', 'comments': 3, 'state': 'closed', 'labels': ['bug', 'triage'], 'assignee': None, 'milestone': None, 'locked': False, 'number': 11084, 'is_pull_request': False})]\n","\n","</s>\n","<|user|>\n","I got an import error: cannot import name 'VectorStoreIndex' from 'llama_index'. How to fix it?\n","</s>\n","<|assistant|>\n","\n","  - The error message indicates that the `VectorStoreIndex` module is not found in the `llama_index` package.\n","  - The issue might be due to the package not being installed correctly or the module being removed in a newer version.\n","  - To fix the issue, you can try the following:\n","    - Check if the package is installed correctly by running `pip list llama_index` or `pip show llama_index`. If the package is not installed, you can install it using `pip install llama_index`.\n","    - Check the package version by running `llama_index --version`. If the version is outdated, you can update it using `pip install --upgrade llama_index`.\n","    - Check the package documentation or the issue tracker on GitHub to see if there are any known issues or updates related to the `VectorStoreIndex` module.\n","    - If none of the above steps work, you can try reinstalling the package or seeking help from the package maintainers or the community.\n","\n","Please provide more information about your environment, such as the operating system, Python version, and the exact error message you are seeing. This will help me provide a more specific solution.assistant|>\n","\n","I'd be happy to help you troubleshoot the issue. Based on the provided context, it seems that you are experiencing an import error when trying to import the `VectorStoreIndex` module from the `llama_index` package.\n","\n","Here are a few potential solutions to try:\n","\n","1. **Check the package version**: Make sure you are using a compatible version of the `llama_index` package. You can check the version by running `llama_index --version`. If the version is outdated, you can update it using `pip install --upgrade llama_index`.\n","2. **Check the package installation**: Ensure that the `llama_index` package is installed correctly. You can check the package installation by running `pip list llama_index` or `pip show llama_index`. If the package is not installed, you can install it using `pip install llama_index`.\n","3. **Check the package documentation**: Review the package documentation to see if there are any known issues or updates related to the `VectorStoreIndex` module.\n","4. **Check the issue tracker**: Look for any open issues or pull requests related to the `VectorStoreIndex` module on the `llama_index` GitHub page. This may provide insight into the issue or a potential fix.\n","5. **Try reinstalling the package**: If none of the above steps work, you can\n"]}],"source":["print(rag_chainn_output)"]},{"cell_type":"markdown","metadata":{},"source":["## Add ColBERT as a reranker\n","ColBERT is a fast and accurate retrieval model, enabling scalable BERT-based search over large text collections in tens of milliseconds.\n","- [RAGatouille](https://python.langchain.com/v0.2/docs/integrations/providers/ragatouille/)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:24:52.216685Z","iopub.status.busy":"2024-06-18T09:24:52.216417Z","iopub.status.idle":"2024-06-18T09:24:57.253000Z","shell.execute_reply":"2024-06-18T09:24:57.252148Z","shell.execute_reply.started":"2024-06-18T09:24:52.216662Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["from ragatouille import RAGPretrainedModel\n","RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:24:57.254854Z","iopub.status.busy":"2024-06-18T09:24:57.254236Z","iopub.status.idle":"2024-06-18T09:24:57.264654Z","shell.execute_reply":"2024-06-18T09:24:57.263648Z","shell.execute_reply.started":"2024-06-18T09:24:57.254825Z"},"trusted":true},"outputs":[],"source":["from typing import List, Optional, Tuple\n","\n","def answer_with_rag(\n","    question: str,\n","    llm: pipeline,\n","    vector_db: FAISS,\n","    reranker: Optional[RAGPretrainedModel],\n","    num_retrieved_docs: int = 5,\n","    num_docs_final: int = 2\n",") -> Tuple[str, List[str]]:\n","    \n","    print(\"=> Retrieving documents...\")\n","    retriever = vector_db.as_retriever(\n","        search_type=\"similarity\",\n","        search_kwargs={\"k\": num_retrieved_docs}\n","    )\n","    relevant_docs = retriever.get_relevant_documents(question)\n","    \n","    relevant_docs_content=[]\n","    for doc in relevant_docs:\n","        page_content = doc.page_content\n","        metadata = doc.metadata\n","        if 'url' in metadata:\n","            url = metadata['url']\n","        else:\n","            url = \"None\"\n","        content = f\"Page Content:\\n{page_content}\\nURL: {url}\\n\"\n","        relevant_docs_content.append(content)\n","        \n","\n","    print(\"=> Reranking documents...\")\n","    relevant_docs = reranker.rerank(question, relevant_docs_content, k=num_docs_final)\n","    relevant_docs_content = [doc[\"content\"] for doc in relevant_docs]\n","\n","    relevant_docs_content = relevant_docs_content[:num_docs_final]\n","\n","    # final prompt\n","    context = \"\\nExtracted documents:\\n\"\n","    context += \"\".join([f\"Document {i}:\\n{doc}\\n\" for i, doc in enumerate(relevant_docs_content)])\n","    final_prompt = prompt_template.format(question=question, context=context)\n","    \n","#     print(\"\\n\\n final_prompt----------------------------------------------------------\")\n","#     print(final_prompt)\n","    \n","    print(\"=> Generating answer...\")    \n","    llm_output = READER_LLM(final_prompt)\n","    \n","    print(\"=> Done...\")\n","    \n","    return llm_output, relevant_docs_content"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:24:57.266204Z","iopub.status.busy":"2024-06-18T09:24:57.265841Z","iopub.status.idle":"2024-06-18T09:25:45.627936Z","shell.execute_reply":"2024-06-18T09:25:45.626974Z","shell.execute_reply.started":"2024-06-18T09:24:57.266177Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["=> Retrieving documents...\n","=> Reranking documents...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00, 12.07it/s]\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["=> Generating answer...\n","=> Done...\n"]}],"source":["answer, relevant_docs = answer_with_rag(\n","    question=question,\n","    llm=READER_LLM,\n","    vector_db=db,\n","    reranker=RERANKER,\n","    num_retrieved_docs=8,\n","    num_docs_final=3,\n",")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T09:25:45.629633Z","iopub.status.busy":"2024-06-18T09:25:45.629297Z","iopub.status.idle":"2024-06-18T09:25:45.635497Z","shell.execute_reply":"2024-06-18T09:25:45.634544Z","shell.execute_reply.started":"2024-06-18T09:25:45.629605Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["======================== Answer ========================\n","\n","\n","  - The error message indicates that the 'VectorStoreIndex' module is not found in the 'llama_index.core' package. This could be due to several reasons:\n","    - The module 'VectorStoreIndex' might have been removed or renamed in the latest version of llama_index.\n","    - The package might not be installed correctly, or there might be a version mismatch.\n","    - The import statement might be incorrect.\n","\n","  - To fix the issue, you can try the following:\n","    - Check the llama_index documentation to see if the 'VectorStoreIndex' module has been removed or renamed.\n","    - Verify that the llama_index package is installed correctly and that you are using the correct version.\n","    - Check the import statement to ensure that it is correct and that the module is being imported from the correct package.\n","    - If the issue persists, you can try reinstalling the package or seeking help from the llama_index community or support team.\n","\n","  - Based on the provided context, it seems that the issue might be related to the version mismatch. The error message suggests that the 'VectorStoreIndex' module is not found in the 'llama_index.core' package, which could be due to the version mismatch.\n","\n","  - To resolve the issue, you can try downgrading the llama_index package to a version that is known to work with the 'VectorStoreIndex' module. Alternatively, you can try to find an alternative module that provides similar functionality.\n","\n","  - If you are still facing issues, please provide more details about your environment, including the operating system, Python version, and the exact error message. This will help me to provide a more accurate solution.\n","\n","==================== Source docs ====================\n","\n","[Document 0]\n","Page Content:\n","### Bug Description\n","\n","I encountered a problem below. I received an error.\n","```\n","from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n","```\n","\n","got some error \n","cannot import name 'VectorStoreIndex' from 'llama_index.core' (unknown location)\n","\n","### Version\n","\n","0.10.11\n","\n","### Steps to Reproduce\n","\n","When I attempted to import, I encountered an error. I suspect that not all core modules were included correctly.\n","\n","### Relevant Logs/Tracbacks\n","URL: https://github.com/run-llama/llama_index/issues/11279\n","\n","\n","[Document 1]\n","Page Content:\n","### Bug Description\n","\n","While running from llama_index import VectorStoreIndex, StorageContext, i got this error  cannot import name 'VectorStoreIndex' from 'llama_index' (/usr/local/lib/python3.10/dist-packages/llama_index/__init__.py)\n","\n","### Version\n","\n","0.9.28.post1\n","\n","### Steps to Reproduce\n","\n","i have no idea how to answer this question \n","\n","### Relevant Logs/Tracbacks\n","\n","_No response_\n","URL: https://github.com/run-llama/llama_index/issues/9948\n","\n","\n","[Document 2]\n","Page Content:\n","### Question Validation\n","\n","- [X] I have searched both the documentation and discord for an answer.\n","\n","### Question\n","\n","I updated today's version to 0.10.1, and found a baffling error.\n","\n","May I know what the problem is?\n","\n","`from llama_index.core import VectorStoreIndex, SimpleDirectoryReader`\n","\n","error msg: ImportError: cannot import name 'VectorStoreIndex' from 'llama_index.core' (unknown location)\n","URL: https://github.com/run-llama/llama_index/issues/10621\n","\n"]}],"source":["print(\"======================== Answer ========================\")\n","print(\"{}\".format(answer[0]['generated_text'].split(\"assistant|>\")[1].split(\"<|system\")[0]))\n","print(\"\\n==================== Source docs ====================\")\n","for i, doc in enumerate(relevant_docs):\n","    print(f\"\\n[Document {i}]\")\n","    print(doc)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"modelInstanceId":28083,"sourceId":33551,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
