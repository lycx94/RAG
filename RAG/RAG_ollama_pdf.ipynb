{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load all PDF files from the \"data\" directory\n",
    "\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "data_dir = \"data\"\n",
    "documents = []\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(data_dir, filename)\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split the text with recursive character-based splitting\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=64\n",
    ")\n",
    "\n",
    "chunked_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bnm04\\rag_ollama_streamlit\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create a FAISS index using the HuggingFace embeddings\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "EMDED_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(model_name=EMDED_MODEL_NAME)\n",
    "\n",
    "db = FAISS.from_documents(chunked_docs, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create a retriever using the FAISS index as the backend\n",
    "TOP_K = 10\n",
    "\n",
    "faiss_retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": TOP_K} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"\"\"How do they prepare the dataset for each data type? \n",
    "# Be as detailed as possible and present the information as a bullet-point list.\"\"\"\n",
    "question = \"\"\"\n",
    "How do they prepare datasets for text, images, and videos? \n",
    "Detail the steps involved, describe preprocessing techniques, and explain data structure and quantity used for pretraining. \n",
    "Present as a bullet-point list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================Top document====================================\n",
      "3 Pre-Training\n",
      "Language model pre-training involves: (1)the curation and ﬁltering of a large-scale training corpus, (2)the\n",
      "development of a model architecture and corresponding scaling laws for determining model size, (3)the\n",
      "development of techniques for eﬃcient pre-training at large scale, and (4)the development of a pre-training\n",
      "recipe. We present each of these components separately below.\n",
      "3.1 Pre-Training Data\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 3}\n",
      "\n",
      "====================================Top document====================================\n",
      "tables and charts. Additionally, we use captions and OCR extractions from existing images to generate\n",
      "additional conversational or question-answer data related to the images.\n",
      "Video.Similar to the image adapter, we use academic datasets with pre-existing annotations and convert them\n",
      "into appropriate textual instructions and target responses. The targets are converted to open-ended responses\n",
      "or multiple-choice options, whichever is more appropriate. We ask humans to annotate videos with questions\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 58}\n",
      "\n",
      "====================================Top document====================================\n",
      "accompanied by a structured representation such as the corresponding markdown or LaTeX notation.\n",
      "Besides improving recognition capabilities of the model for these domains, we ﬁnd this data useful to\n",
      "generate question-answer pairs via the text model for ﬁnetuning.\n",
      "7.1.2 Video Data\n",
      "For video pre-training, we use a large dataset of video-text pairs. Our dataset is curated through a multi-stage\n",
      "process. We ﬁlter and clean the associated texts using rule-based heuristics, such as ensuring a minimum\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 55}\n",
      "\n",
      "====================================Top document====================================\n",
      "still under development and not yet ready for release.\n",
      "Before presenting the results of our experiments in Section 7.6 and 7.7, we describe the data we used to train\n",
      "visual recognition capabilities, the model architecture of the vision components, how we scale training of those\n",
      "components, and our pre-training and post-training recipes.\n",
      "7.1 Data\n",
      "We describe our image and video data separately below.\n",
      "7.1.1 Image Data\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 53}\n",
      "\n",
      "====================================Top document====================================\n",
      "3.1.2 Determining the Data Mix\n",
      "To obtain a high-quality language model, it is essential to carefully determine the proportion of diﬀerent data\n",
      "sources in the pre-training data mix. Our main tools in determining this data mix are knowledge classiﬁcation\n",
      "and scaling law experiments.\n",
      "Knowledge classification. We develop a classiﬁer to categorize the types of information contained in our web\n",
      "data to more eﬀectively determine a data mix. We use this classiﬁer to downsample data categories that are\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 5}\n",
      "\n",
      "====================================Top document====================================\n",
      "3.1 Pre-Training Data\n",
      "We create our dataset for language model pre-training from a variety of data sources containing knowledge\n",
      "until the end of 2023. We apply several de-duplication methods and data cleaning mechanisms on each data\n",
      "source to obtain high-quality tokens. We remove domains that contain large amounts of personally identiﬁable\n",
      "information (PII), and domains with known adult content.\n",
      "3.1.1 Web Data Curation\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 3}\n",
      "\n",
      "====================================Top document====================================\n",
      "7.5.1 Supervised Finetuning Data\n",
      "We describe our supervised ﬁnetuning (SFT) data for image and video capabilities separately below.\n",
      "Image.We utilize a mix of diﬀerent datasets for supervised ﬁnetuning.\n",
      "•Academic datasets. We convert a highly ﬁltered collection of existing academic datasets to question-\n",
      "answer pairs using templates or via LLM rewriting. The LLM rewriting’s purpose is to augment the\n",
      "data with diﬀerent instructions and to improve the language quality of answers.\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 57}\n",
      "\n",
      "====================================Top document====================================\n",
      "Figure 11 Processing file uploads. Example of Llama 3 performing analysis and visualization of an uploaded ﬁle.\n",
      "We follow the principle that post-training should align the model to “know what it knows” rather than add\n",
      "knowledge (Gekhman et al., 2024; Mielke et al., 2020). Our primary approach involves generating data that\n",
      "aligns model generations with subsets of factual data present in the pre-training data. To achieve this, we\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 26}\n",
      "\n",
      "====================================Top document====================================\n",
      "pre-training we are releasing the data generated as part of evaluations with publicly available benchmarks\n",
      "which can be found on Huggingface here. Additional details on our eval setup can be found here.\n",
      "Benchmarks and metrics. Table 16 contains an overview of all the benchmarks, organized by the capability.\n",
      "We apply decontamination of the post-training data by running exact match with the prompts from each\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 33}\n",
      "\n",
      "====================================Top document====================================\n",
      "7.1.1 Image Data\n",
      "Our image encoder and adapter are trained on image-text pairs. We construct this dataset via a complex\n",
      "data processing pipeline that consists of four main stages: (1)quality ﬁltering, (2)perceptual de-duplication,\n",
      "(3)resampling, and (4)optical character recognition. We also apply a series of safety mitigations.\n",
      "•Quality filtering. We implement quality ﬁlters that remove non-English captions and low-quality captions\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bnm04\\rag_ollama_streamlit\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "docs = faiss_retriever.get_relevant_documents(question)\n",
    "\n",
    "for doc in docs:\n",
    "    print(\"\\n====================================Top document====================================\")\n",
    "    print(doc.page_content)\n",
    "    print(\"Metadata:\")\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5: Load and initialize the Llama 3 model\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.1\", base_url=\"http://127.0.0.1:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bnm04\\rag_ollama_streamlit\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Create a prompt template\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Step 7: Create an LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Function to answer questions \n",
    "def answer_question(question, retriever):\n",
    "    # Retrieve relevant documents\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    # Combine the retrieved documents into a single context string\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Generate the answer\n",
    "    response = chain.run(context=context, question=question)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bnm04\\rag_ollama_streamlit\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      "How do they prepare datasets for text, images, and videos? \n",
      "Detail the steps involved, describe preprocessing techniques, and explain data structure and quantity used for pretraining. \n",
      "Present as a bullet-point list.\n",
      "\n",
      "Answer: Here are the steps involved in preparing datasets for text, images, and videos:\n",
      "\n",
      "**Text Data:**\n",
      "\n",
      "* Curation of web data from various sources (until 2023)\n",
      "* Application of de-duplication methods and data cleaning mechanisms to obtain high-quality tokens\n",
      "* Removal of domains containing large amounts of personally identifiable information (PII) and known adult content\n",
      "* Use of captions and OCR extractions from existing images to generate additional conversational or question-answer data related to the images\n",
      "* Conversion of academic datasets with pre-existing annotations into appropriate textual instructions and target responses\n",
      "\n",
      "**Image Data:**\n",
      "\n",
      "* Construction of image-text pairs via a complex data processing pipeline consisting of four main stages:\n",
      "\t+ Quality filtering: removal of non-English captions and low-quality captions\n",
      "\t+ Perceptual de-duplication: removal of duplicate images\n",
      "\t+ Resampling: resizing and reformatting images for training\n",
      "\t+ Optical character recognition (OCR): extraction of text from images\n",
      "* Implementation of quality filters that remove non-English captions and low-quality captions\n",
      "* Use of academic datasets to generate question-answer pairs using templates or LLM rewriting\n",
      "\n",
      "**Video Data:**\n",
      "\n",
      "* Curation of a large dataset of video-text pairs through a multi-stage process\n",
      "* Filtering and cleaning associated texts using rule-based heuristics, such as ensuring a minimum number of relevant words\n",
      "* Annotation of videos with questions accompanied by a structured representation (e.g. markdown or LaTeX notation)\n",
      "* Use of captions and OCR extractions from existing images to generate additional conversational or question-answer data related to the images\n",
      "\n",
      "**Preprocessing Techniques:**\n",
      "\n",
      "* De-duplication methods to remove duplicate data\n",
      "* Data cleaning mechanisms to obtain high-quality tokens\n",
      "* Quality filtering to remove non-English captions, low-quality captions, and other irrelevant data\n",
      "* Perceptual de-duplication to remove duplicate images\n",
      "* Resampling to resize and reformat images for training\n",
      "* Optical character recognition (OCR) to extract text from images\n",
      "\n",
      "**Data Structure and Quantity:**\n",
      "\n",
      "* Image-text pairs with thousands of examples per class\n",
      "* Large-scale video dataset with millions of frames\n",
      "* Pre-training data mix determined by knowledge classification and scaling law experiments\n",
      "* Post-training data aligned to \"know what it knows\" rather than add new knowledge\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "answer = answer_question(question, faiss_retriever)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Retriever\n",
    "Add a BM25Retriever and combine it with the FAISS retriever as an EnsembleRetriever. The EnsembleRetriever will use weighted averaging to combine the results from both retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# initialize the bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(chunked_docs)\n",
    "bm25_retriever.k = TOP_K\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================Top document====================================\n",
      "still under development and not yet ready for release.\n",
      "Before presenting the results of our experiments in Section 7.6 and 7.7, we describe the data we used to train\n",
      "visual recognition capabilities, the model architecture of the vision components, how we scale training of those\n",
      "components, and our pre-training and post-training recipes.\n",
      "7.1 Data\n",
      "We describe our image and video data separately below.\n",
      "7.1.1 Image Data\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 53}\n",
      "\n",
      "====================================Top document====================================\n",
      "7.5.1 Supervised Finetuning Data\n",
      "We describe our supervised ﬁnetuning (SFT) data for image and video capabilities separately below.\n",
      "Image.We utilize a mix of diﬀerent datasets for supervised ﬁnetuning.\n",
      "•Academic datasets. We convert a highly ﬁltered collection of existing academic datasets to question-\n",
      "answer pairs using templates or via LLM rewriting. The LLM rewriting’s purpose is to augment the\n",
      "data with diﬀerent instructions and to improve the language quality of answers.\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 57}\n",
      "\n",
      "====================================Top document====================================\n",
      "intensive human annotation could theoretically resolve these issues, synthetic data generation oﬀers a\n",
      "complementary approach at a lower cost and higher scale, unconstrained by the expertise level of annotators.\n",
      "As such, we use Llama 3 and the code expert to generate a large quantity of synthetic SFT dialogs.\n",
      "We describe three high-level approaches for generating synthetic code data. In total, we generate over 2.7M\n",
      "synthetic examples which were used during SFT.\n",
      "19\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 18}\n",
      "\n",
      "====================================Top document====================================\n",
      "3 Pre-Training\n",
      "Language model pre-training involves: (1)the curation and ﬁltering of a large-scale training corpus, (2)the\n",
      "development of a model architecture and corresponding scaling laws for determining model size, (3)the\n",
      "development of techniques for eﬃcient pre-training at large scale, and (4)the development of a pre-training\n",
      "recipe. We present each of these components separately below.\n",
      "3.1 Pre-Training Data\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 3}\n",
      "\n",
      "====================================Top document====================================\n",
      "tables and charts. Additionally, we use captions and OCR extractions from existing images to generate\n",
      "additional conversational or question-answer data related to the images.\n",
      "Video.Similar to the image adapter, we use academic datasets with pre-existing annotations and convert them\n",
      "into appropriate textual instructions and target responses. The targets are converted to open-ended responses\n",
      "or multiple-choice options, whichever is more appropriate. We ask humans to annotate videos with questions\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 58}\n",
      "\n",
      "====================================Top document====================================\n",
      "data that matches the distribution of speech. These heuristics include focusing on relatively short prompts\n",
      "with a simple structure and without non-text symbols.\n",
      "8.1.2 Speech Generation\n",
      "The speech generation datasets mainly consist of those for training the text normalization (TN) model and\n",
      "the prosody model (PM). Both training data are augmented with an additional input feature of the Llama 3\n",
      "embeddings to provide contextual information.\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 63}\n",
      "\n",
      "====================================Top document====================================\n",
      "accompanied by a structured representation such as the corresponding markdown or LaTeX notation.\n",
      "Besides improving recognition capabilities of the model for these domains, we ﬁnd this data useful to\n",
      "generate question-answer pairs via the text model for ﬁnetuning.\n",
      "7.1.2 Video Data\n",
      "For video pre-training, we use a large dataset of video-text pairs. Our dataset is curated through a multi-stage\n",
      "process. We ﬁlter and clean the associated texts using rule-based heuristics, such as ensuring a minimum\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 55}\n",
      "\n",
      "====================================Top document====================================\n",
      "selected can be used as negative rejected samples and used as additional preference data pairs.\n",
      "7.5.4 Reward Modeling\n",
      "We train a vision reward model (RM) on top of the vision SFT model and the language RM. The vision\n",
      "encoder and the cross-attention layers are initialized from the vision SFT model and unfrozen during training,\n",
      "while the self-attention layers are initialized from the language RM and kept frozen. We observe that freezing\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 59}\n",
      "\n",
      "====================================Top document====================================\n",
      "3.1.2 Determining the Data Mix\n",
      "To obtain a high-quality language model, it is essential to carefully determine the proportion of diﬀerent data\n",
      "sources in the pre-training data mix. Our main tools in determining this data mix are knowledge classiﬁcation\n",
      "and scaling law experiments.\n",
      "Knowledge classification. We develop a classiﬁer to categorize the types of information contained in our web\n",
      "data to more eﬀectively determine a data mix. We use this classiﬁer to downsample data categories that are\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 5}\n",
      "\n",
      "====================================Top document====================================\n",
      "models comprises two main stages:\n",
      "•Language model pre-training. We start by converting a large, multilingual text corpus to discrete tokens\n",
      "and pre-training a large language model (LLM) on the resulting data to perform next-token prediction.\n",
      "In the language model pre-training stage, the model learns the structure of language and obtains large\n",
      "amounts of knowledge about the world from the text it is “reading”. To do this eﬀectively, pre-training\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 2}\n",
      "\n",
      "====================================Top document====================================\n",
      "3.1 Pre-Training Data\n",
      "We create our dataset for language model pre-training from a variety of data sources containing knowledge\n",
      "until the end of 2023. We apply several de-duplication methods and data cleaning mechanisms on each data\n",
      "source to obtain high-quality tokens. We remove domains that contain large amounts of personally identiﬁable\n",
      "information (PII), and domains with known adult content.\n",
      "3.1.1 Web Data Curation\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 3}\n",
      "\n",
      "====================================Top document====================================\n",
      "to perform favorably. We carefully process HTML pages with mathematics and code content to preserve\n",
      "the structure of that content. We maintain the image altattribute text since mathematical content is often\n",
      "represented as pre-rendered images where the math is also provided in the altattribute. We experimentally\n",
      "evaluate diﬀerent cleaning conﬁgurations. We ﬁnd markdown is harmful to the performance of a model that\n",
      "is primarily trained on web data compared to plain text, so we remove all markdown markers.\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 4}\n",
      "\n",
      "====================================Top document====================================\n",
      "managing complexity. We seek to optimize for these three levers in our development process:\n",
      "•Data.Compared to prior versions of Llama (Touvron et al., 2023a,b), we improved both the quantity and\n",
      "qualityofthedataweuseforpre-trainingandpost-training. Theseimprovementsincludethedevelopment\n",
      "of more careful pre-processing and curation pipelines for pre-training data and the development of more\n",
      "rigorous quality assurance and ﬁltering approaches for post-training data. We pre-train Llama 3 on a\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 0}\n",
      "\n",
      "====================================Top document====================================\n",
      "Figure 11 Processing file uploads. Example of Llama 3 performing analysis and visualization of an uploaded ﬁle.\n",
      "We follow the principle that post-training should align the model to “know what it knows” rather than add\n",
      "knowledge (Gekhman et al., 2024; Mielke et al., 2020). Our primary approach involves generating data that\n",
      "aligns model generations with subsets of factual data present in the pre-training data. To achieve this, we\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 26}\n",
      "\n",
      "====================================Top document====================================\n",
      "You are a helpful and cheerful AI Chatbot that acts as a meal plan assistant for busy families.\n",
      "The family consists of 2 adults, 3 teenagers, and 2 preschoolers. Plan two or three days at a time\n",
      "and use leftovers or extra ingredients for the second day’s plan. The user will let you know if they\n",
      "want two or three days. If they don’t, assume three days. Each plan should include breakfast,\n",
      "lunch, snack, and dinner. Ask the user if they approve of the plan or need adjustments. After they\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 27}\n",
      "\n",
      "====================================Top document====================================\n",
      "pre-training we are releasing the data generated as part of evaluations with publicly available benchmarks\n",
      "which can be found on Huggingface here. Additional details on our eval setup can be found here.\n",
      "Benchmarks and metrics. Table 16 contains an overview of all the benchmarks, organized by the capability.\n",
      "We apply decontamination of the post-training data by running exact match with the prompts from each\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 33}\n",
      "\n",
      "====================================Top document====================================\n",
      "data covers a set of diverse domains and realistic APIs. All agents are variants of Llama 3 prompted in\n",
      "diﬀerent ways depending on their roles and collaborate in a step-by-step manner.\n",
      "4.3.6 Factuality\n",
      "Hallucinations remain a major challenge for large language models. Models tend to be overconﬁdent, even in\n",
      "domains where they have little knowledge. Despite these shortcomings, they are often used as knowledge bases,\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 25}\n",
      "\n",
      "====================================Top document====================================\n",
      "7.1.1 Image Data\n",
      "Our image encoder and adapter are trained on image-text pairs. We construct this dataset via a complex\n",
      "data processing pipeline that consists of four main stages: (1)quality ﬁltering, (2)perceptual de-duplication,\n",
      "(3)resampling, and (4)optical character recognition. We also apply a series of safety mitigations.\n",
      "•Quality filtering. We implement quality ﬁlters that remove non-English captions and low-quality captions\n",
      "Metadata:\n",
      "{'source': 'data\\\\llama3.pdf', 'page': 53}\n"
     ]
    }
   ],
   "source": [
    "docs = ensemble_retriever.get_relevant_documents(question)\n",
    "\n",
    "for doc in docs:\n",
    "    print(\"\\n====================================Top document====================================\")\n",
    "    print(doc.page_content)\n",
    "    print(\"Metadata:\")\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      "How do they prepare datasets for text, images, and videos? \n",
      "Detail the steps involved, describe preprocessing techniques, and explain data structure and quantity used for pretraining. \n",
      "Present as a bullet-point list.\n",
      "\n",
      "Answer: Here is the answer:\n",
      "\n",
      "**Preparing Datasets for Text, Images, and Videos:**\n",
      "\n",
      "* **Text Data:**\n",
      "\t+ Create a dataset from various sources containing knowledge until 2023\n",
      "\t+ Apply de-duplication methods and data cleaning mechanisms on each data source to obtain high-quality tokens\n",
      "\t+ Remove domains with personally identifiable information (PII) or adult content\n",
      "\t+ Process HTML pages with mathematics and code content to preserve the structure of that content\n",
      "* **Image Data:**\n",
      "\t+ Construct a dataset via a complex data processing pipeline:\n",
      "\t\t- Quality filtering: remove non-English captions and low-quality captions\n",
      "\t\t- Perceptual de-duplication: group similar images together\n",
      "\t\t- Resampling: resize images to uniform size\n",
      "\t\t- Optical character recognition (OCR): extract text from images\n",
      "\t+ Apply a series of safety mitigations\n",
      "\n",
      "Note that the steps for video data preparation are not explicitly mentioned, but it's likely that they involve a combination of techniques used for text and image data, such as quality filtering, perceptual de-duplication, resampling, and OCR.\n",
      "\n",
      "**Preprocessing Techniques:**\n",
      "\n",
      "* De-duplication methods\n",
      "* Data cleaning mechanisms\n",
      "* Quality filtering\n",
      "* Perceptual de-duplication\n",
      "* Resampling\n",
      "* Optical character recognition (OCR)\n",
      "\n",
      "**Data Structure and Quantity Used for Pretraining:**\n",
      "\n",
      "* The dataset is created from various sources containing knowledge until 2023\n",
      "* The data is processed using a complex pipeline to obtain high-quality tokens\n",
      "* The exact quantity of the dataset is not specified, but it's mentioned that it's an improvement over prior versions of Llama.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "answer = answer_question(question, ensemble_retriever)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## semantic chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "data_folder = \"data\"\n",
    "loader = DirectoryLoader(data_folder, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "semantic_chunker = SemanticChunker(embed_model, breakpoint_threshold_type=\"standard_deviation\")\n",
    "semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
    "semantic_chunk_vectorstore = FAISS.from_documents(semantic_chunks, embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_chunk_retriever = semantic_chunk_vectorstore.as_retriever(search_kwargs={\"k\" : TOP_K})\n",
    "docs = semantic_chunk_retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================Top document====================================\n",
      "Figure 11 Processing file uploads. Example of Llama 3 performing analysis and visualization of an uploaded ﬁle. We follow the principle that post-training should align the model to “know what it knows” rather than add\n",
      "knowledge (Gekhman et al., 2024; Mielke et al., 2020). Our primary approach involves generating data that\n",
      "aligns model generations with subsets of factual data present in the pre-training data. To achieve this, we\n",
      "develop a knowledge probing technique that takes advantage of Llama 3’s in-context abilities. This data\n",
      "generation process involves the following procedure:\n",
      "1.Extract a data snippet from the pre-training data. 2.Generate a factual question about these snippets (context) by prompting Llama 3\n",
      "3.Sample responses from Llama 3 to the question\n",
      "4.Score the correctness of the generations using the original context as a reference and Llama 3 as a judge\n",
      "5.Score the informativeness of the generations using Llama 3 as a judge\n",
      "6.Generate a refusal for responses which are consistently informative and incorrect across the generations,\n",
      "using Llama 3\n",
      "We use data generated from the knowledge probe to encourage the model to only answer questions which it\n",
      "has knowledge about, and refuse answering those questions that it is unsure about. Further, pre-training data\n",
      "is not always factually consistent or correct. We therefore also collect a limited set of labeled factuality data\n",
      "that deals with sensitive topics where factually contradictory or incorrect statements are prevalent. 27\n",
      "\n",
      "====================================Top document====================================\n",
      "8.1 Data\n",
      "8.1.1 Speech Understanding\n",
      "The training data can be categorized into two types. The pre-training data includes a large amount of\n",
      "unlabeled speech, which is used to initialize the speech encoder in a self-supervised manner. The supervised\n",
      "ﬁnetuning data includes speech recognition, speech translation, and spoken dialogue data; this data is used to\n",
      "unlock speciﬁc abilities when integrated with the large language model. Pre-training data. To pre-train the speech encoder, we curate a dataset of approximately 15M hours of speech\n",
      "recordings encompassing a large number of languages. We ﬁlter our audio data using a voice activity detection\n",
      "(VAD) model and select audio samples with a VAD threshold above 0.7 for pre-training. In speech pre-training\n",
      "data, we also focus on ensuring the absence of PII. We use the Presidio Analyzer to identify such PII. Speech recognition and translation data. Our ASR training data contains 230K hours of manually transcribed\n",
      "speech recordings that span 34 languages. Our AST training data contains 90K hours of translations in\n",
      "two directions: from 33 languages to English and from English to 33 languages. This data contains both\n",
      "supervised and synthetic data generated using the NLLB toolkit (NLLB Team et al., 2022). The use of\n",
      "synthetic AST data enables us to increase model quality for low-resource languages. The speech segments in\n",
      "our data have a maximum length of 60 seconds. Spoken dialogue data. To ﬁnetune the speech adapter for spoken dialogue, we synthetically generate responses\n",
      "18The speech interface supports the following 34 languages: Arabic, Bengali, Chinese, Czech, Dutch, English, Finnish, French,\n",
      "German, Greek, Gujarati, Hindi, Hungarian, Indonesian, Italian, Japanese, Kannada, Korean, Malayalam, Marathi, Persian,\n",
      "Polish, Portuguese, Romanian, Russian, Spanish, Swahili, Swedish, Tamil, Telugu, Thai, Turkish, Urdu, Vietnamese. 63\n",
      "\n",
      "====================================Top document====================================\n",
      "neighbors. Annotators are also provided with intermediate checkpoints of existing models to facilitate\n",
      "model-in-the-loop style annotations, so that model generations can be utilized as a starting point by\n",
      "the annotators to then provide additional human edits. This is an iterative process, in which model\n",
      "checkpoints would be regularly updated with better performing versions trained on the latest data. This\n",
      "increases the volume and eﬃciency of human annotations, while also improving their quality. •Synthetic data. We explore diﬀerent ways to generate synthetic multi-modal data by using text-\n",
      "representations of images and a text-input LLM. The high-level idea is to utilize the reasoning capa-\n",
      "bilities of text-input LLMs to generate question-answer pairs in the text domain, and replace the text\n",
      "representation with its corresponding images to produce synthetic multi-modal data. Examples include\n",
      "rendering texts from question-answer datasets as images or rendering table data into synthetic images of\n",
      "tables and charts. Additionally, we use captions and OCR extractions from existing images to generate\n",
      "additional conversational or question-answer data related to the images. Video.Similar to the image adapter, we use academic datasets with pre-existing annotations and convert them\n",
      "into appropriate textual instructions and target responses. The targets are converted to open-ended responses\n",
      "or multiple-choice options, whichever is more appropriate. We ask humans to annotate videos with questions\n",
      "and corresponding answers. The annotators are asked to focus on questions that could not be answered based\n",
      "on a single frame, to steer the annotators towards questions that require temporal understanding. 7.5.2 Supervised Finetuning Recipe\n",
      "We describe our supervised ﬁnetuning (SFT) recipe for image and video capabilities separately below. Image.We initialize from the pre-trained image adapter, but hot-swap the pre-trained language model’s\n",
      "weights with the instruction tuned language model’s weights. The language model weights are kept frozen to\n",
      "maintain text-only performance, i.e., we only update the vision encoder and image adapter weights. Our approach to ﬁnetune the model is similar to Wortsman et al.\n",
      "\n",
      "====================================Top document====================================\n",
      "Moeslund. Foundation models for\n",
      "video understanding: A survey. 2024. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe,\n",
      "and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European\n",
      "Conference on Computer Vision (ECCV) , September 2018. Soumi Maiti, Yifan Peng, Shukjae Choi, Jee weon Jung, Xuankai Chang, and Shinji Watanabe. Voxtlm: uniﬁed\n",
      "decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks. 2023. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shaﬁq Joty, and Enamul Hoque. ChartQA: A benchmark for question\n",
      "answering about charts with visual and logical reasoning. In Smaranda Muresan, Preslav Nakov, and Aline\n",
      "Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022 , pages 2263–2279,\n",
      "Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.ﬁndings-acl.177. https://aclanthology.org/2022.ﬁndings-acl.177 . Minesh Mathew, Dimosthenis Karatzas, R. Manmatha, and C. V. Jawahar. Docvqa: A dataset for vqa on document\n",
      "images. 2021 IEEE Winter Conference on Applications of Computer Vision (WACV) , pages 2199–2208, 2020. https://api.semanticscholar.org/CorpusID:220280200 . Jeremy Baumgartner Matt Bowman. Meta open compute project, grand teton ai platform, 2022. https://engineering. fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/ . Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh,\n",
      "Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et al. Openelm: An eﬃcient language model family with open-source\n",
      "training and inference framework. arXiv preprint arXiv:2404.14619 , 2024. Dheeraj Mekala, Jason Weston, Jack Lanchantin, Roberta Raileanu, Maria Lomeli, Jingbo Shang, and Jane Dwivedi-Yu. Toolveriﬁer: Generalization to new tools via self-veriﬁcation. arXiv preprint arXiv:2402.14158 , 2024. 83\n",
      "\n",
      "====================================Top document====================================\n",
      "Le, and Jason\n",
      "Wei. Scaling instruction-ﬁnetuned language models. CoRR, abs/2210.11416, 2022. doi: 10.48550/ARXIV.2210.11416. https://doi.org/10.48550/arXiv.2210.11416 . Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 ,\n",
      "2018. 77\n",
      "\n",
      "====================================Top document====================================\n",
      "Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for alignment? a comprehensive\n",
      "study of automatic data selection in instruction tuning, 2024c. https://arxiv.org/abs/2312.15685 . Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\n",
      "and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 ,\n",
      "2019a. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\n",
      "and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019b. http://arxiv.org/abs/1907.11692 . Llama-Team. Meta llama guard 2. https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/\n",
      "MODEL_CARD.md , 2024. Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Instag:\n",
      "Instruction tagging for analyzing supervised ﬁne-tuning of large language models, 2023. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and\n",
      "where to ﬁnd them: Overcoming few-shot prompt order sensitivity. In Smaranda Muresan, Preslav Nakov, and\n",
      "Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers) , pages 8086–8098, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556. https://aclanthology.org/2022.acl-long.556 . Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng\n",
      "Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via\n",
      "reinforced evol-instruct. arXiv preprint arXiv:2308.09583 , 2023. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed\n",
      "video understanding via large vision and language models. In ACL, 2024. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreﬀe, Uri Alon, Nouha Dziri,\n",
      "Shrimai Prabhumoye, Yiming Yang, et al. Self-reﬁne: Iterative reﬁnement with self-feedback. Advances in Neural\n",
      "Information Processing Systems , 36, 2024a. Lovish Madaan, Aaditya K Singh, Rylan Schaeﬀer, Andrew Poulton, Sanmi Koyejo, Pontus Stenetorp, Sharan Narang,\n",
      "and Dieuwke Hupkes. Quantifying variance in evaluation benchmarks. arXiv preprint arXiv:2406.10229 , 2024b. Neelu Madan, Andreas Moegelmose, Rajat Modi, Yogesh S. Rawat, and Thomas B.\n",
      "\n",
      "====================================Top document====================================\n",
      "•We apply language-speciﬁc heuristics and model-based ﬁlters to remove low-quality documents. In addition, we perform quality ranking of multilingual documents using a multilingual Llama 2-based classiﬁer\n",
      "to ensure that high-quality content is prioritized. We determine the amount of multilingual tokens used in\n",
      "pre-training experimentally, balancing model performance on English and multilingual benchmarks. 3.1.2 Determining the Data Mix\n",
      "To obtain a high-quality language model, it is essential to carefully determine the proportion of diﬀerent data\n",
      "sources in the pre-training data mix. Our main tools in determining this data mix are knowledge classiﬁcation\n",
      "and scaling law experiments. Knowledge classification. We develop a classiﬁer to categorize the types of information contained in our web\n",
      "data to more eﬀectively determine a data mix. We use this classiﬁer to downsample data categories that are\n",
      "over-represented on the web, for example, arts and entertainment. Scaling laws for data mix. To determine the best data mix, we perform scaling law experiments in which we\n",
      "train several small models on a data mix and use that to predict the performance of a large model on that mix\n",
      "(see Section 3.2.1). We repeat this process multiple times for diﬀerent data mixes to select a new data mix\n",
      "candidate. Subsequently, we train a larger model on this candidate data mix and evaluate the performance of\n",
      "that model on several key benchmarks. Data mix summary. Our ﬁnal data mix contains roughly 50% of tokens corresponding to general knowledge,\n",
      "25% of mathematical and reasoning tokens, 17% code tokens, and 8% multilingual tokens. 3.1.3 Annealing Data\n",
      "Empirically, we ﬁnd that annealing (see Section 3.4.3) on small amounts of high-quality code and mathematical\n",
      "data can boost the performance of pre-trained models on key benchmarks. Akin to Li et al. (2024b), we\n",
      "perform annealing with a data mix that upsamples high-quality data in select domains. We do not include\n",
      "any training sets from commonly used benchmarks in our annealing data. This enables us to assess the true\n",
      "few-shot learning capabilities and out-of-domain generalization of Llama 3. Following OpenAI (2023a), we evaluate the eﬃcacy of annealing on the GSM8k (Cobbe et al., 2021) and\n",
      "MATH (Hendrycks et al., 2021b) training sets in annealing. We ﬁnd that annealing improved the performance\n",
      "of a pre-trained Llama 3 8B model on the GSM8k and MATH validation sets by 24.0% and 6.4%, respectively. However, the improvements on the 405B model are negligible, suggesting that our ﬂagship model has strong\n",
      "in-context learning and reasoning capabilities and does not require speciﬁc in-domain training samples to\n",
      "obtain strong performance. Using annealing to assess data quality. Similar to Blakeney et al. (2024), we ﬁnd that annealing enables us to\n",
      "judge the value of small domain-speciﬁc datasets. We measure the value of such datasets by annealing the\n",
      "learning rate of a 50% trained Llama 3 8B model linearly to 0 on 40B tokens. In those experiments, we assign\n",
      "30% weight to the new dataset and the remaining 70% weight to the default data mix. Using annealing to\n",
      "evaluate new data sources is more eﬃcient than performing scaling law experiments for every small dataset. 3.2 Model Architecture\n",
      "Llama 3 uses a standard, dense Transformer architecture (Vaswani et al., 2017). It does not deviate signiﬁcantly\n",
      "from Llama and Llama 2 (Touvron et al., 2023a,b) in terms of model architecture; our performance gains are\n",
      "primarily driven by improvements in data quality and diversity as well as by increased training scale. We do make a few smaller modiﬁcations compared to Llama 3:\n",
      "•We use grouped query attention (GQA; Ainslie et al. (2023)) with 8 key-value heads to improve inference\n",
      "speed and to reduce the size of key-value caches during decoding. •We use an attention mask that prevents self-attention between diﬀerent documents within the same\n",
      "sequence. We ﬁnd that this change had limited impact during in standard pre-training, but ﬁnd it to be\n",
      "important in continued pre-training on very long sequences. 6\n",
      "\n",
      "====================================Top document====================================\n",
      "Data heterogeneity. The data is heterogeneous because, on average, images have more tokens than the\n",
      "associated text: an image has 2,308 tokens, whereas the associated text contains an average of only 192 tokens. As a result, the computation of cross-attention layers requires more time and memory than the computation\n",
      "of self-attention layers. We address this problem by introducing sequence parallelization in the image encoder,\n",
      "so that each GPU processes roughly the same number of tokens. Because the average text size is relatively\n",
      "short, we also use a substantially larger micro-batch size (8 instead of 1). Numerical instabilities. After the image encoder is added to the model, we ﬁnd that performing gradient\n",
      "accumulation in bf16 led to numerical instabilities. The most likely explanation for this is that image tokens\n",
      "are introduced into the language backbone via allcross-attention layers. This implies that numerical deviations\n",
      "in the representation of an image token have an outsized impact on the overall computation because the errors\n",
      "are compounded. We address this by performing gradient accumulation in FP32. 7.4 Pre-training\n",
      "Image.We initialize from the pre-trained text model and vision encoder weights. The vision encoder is\n",
      "unfrozen, while the text model weights are kept frozen as explained above. First, we train the model using 6B\n",
      "image-text pairs where each image is resized to ﬁt within four tiles of 336×336pixels. We use a global batch\n",
      "size of 16,384 and a cosine learning rate schedule with initial learning rate 10×10−4and a weight decay of\n",
      "0.01. The initial learning rate was determined based on small-scale experiments. However, these ﬁndings did\n",
      "not generalize well to very long training schedules and dropped the learning rate a few times during training\n",
      "when the loss values became stagnant. After the base pre-training, we increase the image resolution further\n",
      "and continue training the same weights on the annealing dataset. The optimizer is re-initialized via warm-up\n",
      "to learning rate 2×10−5and again follows a cosine schedule. Video.For video pre-training, we start from the image pre-trained and annealed weights as described above. We\n",
      "add the video aggregator and cross-attention layers as described in the architecture, initialized randomly. We\n",
      "freeze all the parameters in the model except the video-speciﬁc ones (the aggregator and video cross-attention),\n",
      "and train them on the video pre-training data. We use the same training hyperparameters as the image\n",
      "annealing stage, with small diﬀerences in the learning rate. We uniformly sample 16 frames from the full video,\n",
      "and represent each frame using four chunks, each of size of 448×448pixels. We use an aggregation factor of\n",
      "16 in the video aggregator, hence obtaining one eﬀective frame, which the text tokens cross-attend to. We use\n",
      "a global batch size of 4,096, a sequence length of 190 tokens, and a learning rate of 10−4during training. 7.5 Post-Training\n",
      "In this section, we describe the post-training recipe for our vision adapters. After pre-training, we ﬁne-tune the\n",
      "model on highly curated multi-modal conversational data to enable chat capabilities. We further implement\n",
      "direct preference optimization (DPO) to boost human evaluation performance and rejection sampling to\n",
      "improve multi-modal reasoning capabilities. Finally, we add a quality-tuning stage where we continue ﬁne-\n",
      "tuning the model on a very small set of high-quality conversational data which further boosts human evaluation\n",
      "while retaining performance across benchmarks. More details on each of these steps are provided below. 7.5.1 Supervised Finetuning Data\n",
      "We describe our supervised ﬁnetuning (SFT) data for image and video capabilities separately below. Image.We utilize a mix of diﬀerent datasets for supervised ﬁnetuning. •Academic datasets. We convert a highly ﬁltered collection of existing academic datasets to question-\n",
      "answer pairs using templates or via LLM rewriting. The LLM rewriting’s purpose is to augment the\n",
      "data with diﬀerent instructions and to improve the language quality of answers. •Human annotations. We collect multi-modal conversation data via human annotators for a wide range of\n",
      "tasks (open-ended question-answering, captioning, practical use cases, etc.) and domains ( e.g., natural\n",
      "images and structured images). Annotators are provided with images and asked to write conversations. To ensure diversity, we cluster large-scale datasets and sampled images uniformly across diﬀerent clusters. Further, we acquire additional images for a few speciﬁc domains by expanding a seed via k-nearest\n",
      "58\n",
      "\n",
      "====================================Top document====================================\n",
      "Figure 1 Illustration of the overall architecture and training of Llama 3. Llama 3 is a Transformer language model trained to\n",
      "predict the next token of a textual sequence. See text for details. self-supervised approach that masks out parts of the speech inputs and tries to reconstruct the masked\n",
      "out parts via a discrete-token representation. As a result, the model learns the structure of speech\n",
      "signals. See Section 7 for details on the image encoder and Section 8 for details on the speech encoder. •Vision adapter training. We train an adapter that integrates the pre-trained image encoder into the\n",
      "pre-trained language model. The adapter consists of a series of cross-attention layers that feed image-\n",
      "encoder representations into the language model. The adapter is trained on text-image pairs. This\n",
      "aligns the image representations with the language representations. During adapter training, we also\n",
      "update the parameters of the image encoder but we intentionally do not update the language-model\n",
      "parameters. We also train a video adapter on top of the image adapter on paired video-text data. This\n",
      "enables the model to aggregate information across frames. See Section 7 for details. •Speech adapter training. Finally, we integrate the speech encoder into the model via an adapter that\n",
      "converts speech encodings into token representations that can be fed directly into the ﬁnetuned language\n",
      "model. The parameters of the adapter and encoder are jointly updated in a supervised ﬁnetuning stage\n",
      "to enable high-quality speech understanding. We do not change the language model during speech\n",
      "adapter training. We also integrate a text-to-speech system. See Section 8 for details. Our multimodal experiments lead to models that can recognize the content of images and videos, and support\n",
      "interaction via a speech interface. These models are still under development and not yet ready for release. 3 Pre-Training\n",
      "Language model pre-training involves: (1)the curation and ﬁltering of a large-scale training corpus, (2)the\n",
      "development of a model architecture and corresponding scaling laws for determining model size, (3)the\n",
      "development of techniques for eﬃcient pre-training at large scale, and (4)the development of a pre-training\n",
      "recipe. We present each of these components separately below. 3.1 Pre-Training Data\n",
      "We create our dataset for language model pre-training from a variety of data sources containing knowledge\n",
      "until the end of 2023. We apply several de-duplication methods and data cleaning mechanisms on each data\n",
      "source to obtain high-quality tokens. We remove domains that contain large amounts of personally identiﬁable\n",
      "information (PII), and domains with known adult content. 3.1.1 Web Data Curation\n",
      "Much of the data we utilize is obtained from the web and we describe our cleaning process below. PII and safety filtering. Among other mitigations, we implement ﬁlters designed to remove data from websites\n",
      "are likely to contain unsafe content or high volumes of PII, domains that have been ranked as harmful\n",
      "according to a variety of Meta safety standards, and domains that are known to contain adult content. 4\n",
      "\n",
      "====================================Top document====================================\n",
      "Figure 27 Throughput-latency trade-off in FP8 inference with Llama 3 405B compared with BF16 inference using diﬀerent\n",
      "pipeline parallelization setups. Left:Results for pre-ﬁlling. Right:Results for decoding. 7 Vision Experiments\n",
      "We perform a series of experiments in which we incorporate visual-recognition capabilities into Llama 3 via\n",
      "a compositional approach that consists of two main stages. First, we compose a pre-trained image encoder\n",
      "(Xu et al., 2023) and the pre-trained language model by introducing and training a set of cross-attention\n",
      "layers between the two models (Alayrac et al., 2022) on a large number of image-text pairs. This leads to\n",
      "the model illustrated in Figure 28. Second, we introduce temporal aggregator layers and additional video\n",
      "cross-attention layers that operate on a large collection of video-text pairs to learn the model to recognize and\n",
      "process temporal information from videos. A compositional approach to foundation model development has several advantages: (1)it enables us to\n",
      "parallelize the development of the vision and language modeling capabilities; (2)it circumvents complexities\n",
      "of joint pre-training on visual and language data that stem from tokenization of visual data, diﬀerences in\n",
      "background perplexities of tokens originating from diﬀerent modalities, and contention between modalities; (3)\n",
      "it guarantees that model performance on text-only tasks is not aﬀected by the introduction of visual-recognition\n",
      "capabilities, and (4)the cross-attention architecture ensures that we do not have to expend compute passing\n",
      "full-resolution images through the increasingly LLM backbones (speciﬁcally, the feed-forward networks in\n",
      "each transformer layer), making it more eﬃcient during inference. We note that our multimodal models are\n",
      "still under development and not yet ready for release. Before presenting the results of our experiments in Section 7.6 and 7.7, we describe the data we used to train\n",
      "visual recognition capabilities, the model architecture of the vision components, how we scale training of those\n",
      "components, and our pre-training and post-training recipes. 7.1 Data\n",
      "We describe our image and video data separately below. 7.1.1 Image Data\n",
      "Our image encoder and adapter are trained on image-text pairs. We construct this dataset via a complex\n",
      "data processing pipeline that consists of four main stages: (1)quality ﬁltering, (2)perceptual de-duplication,\n",
      "(3)resampling, and (4)optical character recognition. We also apply a series of safety mitigations. •Quality filtering. We implement quality ﬁlters that remove non-English captions and low-quality captions\n",
      "via heuristics such as low alignment scores produced by (Radford et al., 2021). Speciﬁcally, we remove\n",
      "all image-text pairs below a certain CLIP score. •De-duplication. De-duplicating large-scale training datasets beneﬁts model performance because it\n",
      "reduces training compute spent on redundant data (Esser et al., 2024; Lee et al., 2021; Abbas et al.,\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(\"\\n====================================Top document====================================\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      "How do they prepare datasets for text, images, and videos? \n",
      "Detail the steps involved, describe preprocessing techniques, and explain data structure and quantity used for pretraining. \n",
      "Present as a bullet-point list.\n",
      "\n",
      "Answer: Here are the steps involved in preparing datasets for text, images, and videos:\n",
      "\n",
      "**Text Dataset Preparation:**\n",
      "\n",
      "* Curate and filter a large-scale training corpus from various data sources\n",
      "* Remove domains with personally identifiable information (PII) and adult content\n",
      "* Apply de-duplication methods to remove redundant data\n",
      "* Clean and preprocess the data using techniques such as tokenization and filtering\n",
      "\n",
      "**Image Dataset Preparation:**\n",
      "\n",
      "* Construct image-text pairs through a complex data processing pipeline consisting of:\n",
      "\t+ Quality filtering: Remove non-English captions, low-quality captions, and image-text pairs with low alignment scores\n",
      "\t+ Perceptual de-duplication: Reduce redundant data to improve model performance\n",
      "\t+ Resampling: Adjust the resolution of images as needed\n",
      "\t+ Optical character recognition (OCR): Extract text from images if necessary\n",
      "* Apply safety mitigations to remove potentially hazardous content\n",
      "\n",
      "**Video Dataset Preparation:**\n",
      "\n",
      "* Construct video-text pairs through a process similar to image dataset preparation, with additional considerations for temporal information and video processing\n",
      "* Apply de-duplication methods to reduce redundant data\n",
      "* Clean and preprocess the data using techniques such as tokenization and filtering\n",
      "\n",
      "**Preprocessing Techniques:**\n",
      "\n",
      "* Tokenization: Breaking down text into individual tokens (e.g., words or subwords) for processing\n",
      "* Filtering: Removing unwanted characters, punctuation, or special characters from text or image captions\n",
      "* De-duplication: Identifying and removing redundant data to improve model performance\n",
      "* Resampling: Adjusting the resolution of images as needed\n",
      "\n",
      "**Data Structure and Quantity Used for Pretraining:**\n",
      "\n",
      "* Text dataset: Not specified in detail, but mentioned to be a large-scale corpus with millions of tokens\n",
      "* Image dataset: Millions of image-text pairs, processed through a complex data pipeline to remove redundant and low-quality data\n",
      "* Video dataset: Similar to the image dataset, with additional considerations for temporal information and video processing\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "answer = answer_question(question, semantic_chunk_retriever)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "1. The ensemble retriever with BM25 did not improve performance and lacks depth and video data. \n",
    "2. Faiss, with either RecursiveCharacterTextSplitter or SemanticTextSplitter, provides a clear, structured method for preparing text, image, and video data, enhancing understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
